---
title: Transformer Models
tags:
keywords: "topic" # new keywords requiere to create a new tag file
last_updated: "June 1, 2020"
summary: 
published: true
sidebar: neural_networks_sidebar #name of yml sidebar file withouth extension
permalink: transformers.html
folder: neural_networks
---



{% include note.html content="Please utilize the template below as a reference for your contribution. Adapt the template when deemed necessary" %}

## What are Transformers?

Brief description.


## Recommended Path for Learning

* This talk by Leo Dirac is a great starting point for those familiar with other neural language models. He gives a brief recap on the history of bag-of-words, RNNs, and LSTMs before diving into what makes the Transformer architecture so special. 
<iframe width="1100" height="550" src="https://www.youtube.com/embed/S27pHKBEp30" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

* This blogpost provides a high level overview of the Transformer architecture highlighting the importance of features like positional encoding.  
<a href="http://jalammar.github.io/illustrated-transformer/" target="_blank"> The illustrated Transformer.</a>

* If you want to begin to get your hands dirty working with these models, here's a google colab notebook that implements the original Transformer model from the *Attention is all you need* paper.  
<a href="https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb" target="_blank"> Attention is all you need notebook.</a>

## Further Learning

## Video

* Video 1
* Video 2

## Applied papers 

* Paper 1
* Paper 2

## Online tutorials

* Online tutorial 1
* Online tutorial 2

## Theory papers 
* Paper 1
* Paper 2

{% include links.html %}
