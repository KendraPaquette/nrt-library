---
title: Activation Functions
tags:
keywords: "topic" # new keywords requiere to create a new tag file
last_updated: "June 1, 2020"
summary: "Activation functions."
published: true
sidebar: neural_networks_sidebar #name of yml sidebar file withouth extension
permalink: activation_functions.html
folder: neural_networks
---


{% include note.html content="Please utilize the template below as a reference for your contribution. Adapt the template when deemed necessary" %}

## What is TOPIC-NAME?

Brief description.


## Recommended Path for Learning

* [What is Activation Function?](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6) This Towards Data Science is a great post explaining activation function in neural networks. 

* This 8 minute video introduces the various activation functions along with their advantages, disadvantages, and when to apply such functions
<iframe width="560" height="315" src="https://www.youtube.com/embed/-7scQpJT7uo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Further Learning

## Applied papers 

* This paper aims to analyze the performance of generalized MLP architectures which has back-propagation algorithm using various different activation functions for the neurons of hidden and output layers. For experimental comparisons, Bi-polar sigmoid, Uni-polar sigmoid, Tanh, Conic Section, and Radial Bases Function (RBF) were used. [Performance Analysis of Various Activation Functions in Generalized MLP Architectures of Neural Networks](http://www.cscjournals.org/manuscript/Journals/IJAE/Volume1/Issue4/IJAE-26.pdf)

* [ANALYSIS OF DIFFERENT ACTIVATION FUNCTIONS USING BACK PROPAGATION NEURAL NETWORKS](http://www.jatit.org/volumes/Vol47No3/61Vol47No3.pdf) This paper aims to analyze and compare different activation functions using backpropagation in a neural network. The purpose is to figure out the optimal activation function for a problem.



{% include links.html %}
